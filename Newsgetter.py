####################################################################################################################################
####################################################################################################################################
########################################################## Version 1.0.0 ###########################################################
####################################################################################################################################
####################################################################################################################################
# Import Libraries
####################################################################################################################################
import streamlit as st
from streamlit_tags import st_tags, st_tags_sidebar
from stqdm import stqdm
import pandas as pd
import io

from Scrappers import *
from Parameters import *
####################################################################################################################################


####################################################################################################################################
# Identify search parameters and get user inputs
####################################################################################################################################
#-------------------------------------------------------------------------------------------------------------------------------
# Styling:
#-------------------------------------------------------------------------------------------------------------------------------

st.title('Get Started with :blue[Newsgetter]')
st.markdown("This tool was created to streamline the creation of newsletters by leveraging Google News")
st.caption('Newsgetter - v1.0.0')
st.subheader('Create a Newsletter',divider='rainbow')
#-------------------------------------------------------------------------------------------------------------------------------



#-------------------------------------------------------------------------------------------------------------------------------
# User Inputs
#-------------------------------------------------------------------------------------------------------------------------------
countries = st.multiselect('Countries*',list(Google_Parameters.countries().keys())[1:])
included_keywords = st_tags(label='Included Keywords *', text='Press enter to add more', maxtags = 100, key='Included Keywords')
exclude_keywords= st_tags(label='Excluded Keywords', text='Press enter to add more', maxtags = 100, key='Excluded Keywords')
websites= st_tags(label='Websites & Domains', text='Press enter to add more', maxtags = 100, key='Websites')
#-------------------------------------------------------------------------------------------------------------------------------

#-------------------------------------------------------------------------------------------------------------------------------
st.divider()

col1, col2 = st.columns(2)
with col1:
    language = st.selectbox('Language *',Google_Parameters.interface_langs().keys(),placeholder="Choose a language",help='Language of results')
    location = st.selectbox('Location',Google_Parameters.countries().keys(),placeholder="Choose a location",help='Geographical location of search')
#-------------------------------------------------------------------------------------------------------------------------------    
with col2:
    date_range = st.date_input ('Date Range *',value=[],format="DD/MM/YYYY",help='Select start and end date for the search')
    quantity = st.number_input('Results Quantity',min_value=10,max_value=100,help='Number of results per country')
    
st.divider()
#-------------------------------------------------------------------------------------------------------------------------------



####################################################################################################################################
# Extract all avaiable results
####################################################################################################################################
#-------------------------------------------------------------------------------------------------------------------------------
# Creaste Session to save data
#-------------------------------------------------------------------------------------------------------------------------------
Session = st.session_state
col3,col4,col5,col6 = st.columns(4)
#-------------------------------------------------------------------------------------------------------------------------------



#-------------------------------------------------------------------------------------------------------------------------------
# Start Search
#-------------------------------------------------------------------------------------------------------------------------------
with col3:
    Start_Search = st.button ('Start Search')
  
if Start_Search:
    #-------------------------------------------------------------------------------------------------------------------------------
    # Styling
    #-------------------------------------------------------------------------------------------------------------------------------
    st.divider()
    st.subheader('Google Results',divider='rainbow')
    #-------------------------------------------------------------------------------------------------------------------------------
    
    
    try:
        #-------------------------------------------------------------------------------------------------------------------------------
        # Prepare Google Search Links
        #-------------------------------------------------------------------------------------------------------------------------------
        Google_Links = []
        for country in countries:
            Start_Date = date_range[0].strftime("%m-%d-%Y") if date_range else ""
            End_Date = date_range[1].strftime("%m-%d-%Y")  if date_range else ""
            Link = Scrappers.Google_Advanced_URL(query=country,exact_words=included_keywords, none_of_words=exclude_keywords, site_or_domain=websites, results_count=quantity, term_apperaing='',
                                                start_date=Start_Date, end_date=End_Date, country=location, search_language=language, search_type='')
            st.write(f"Search started for {country} at: [Link]({Link.replace(' ','+')})")
            Google_Links.append({country:Link})
        #-------------------------------------------------------------------------------------------------------------------------------
        
        
        
        #-------------------------------------------------------------------------------------------------------------------------------
        # Get Google Links Results
        #-------------------------------------------------------------------------------------------------------------------------------
        Google_Results = pd.DataFrame()
        for result in stqdm(Google_Links):
            Country = list(result.keys())[0]
            Link = result[Country]
            Link_Results = Scrappers.Google_News_Selenium(Link)
            Link_Results['Country'] = Country
            Google_Results = pd.concat([Google_Results,Link_Results],ignore_index=True)
        
        st.data_editor(Google_Results,column_config={"Link": st.column_config.LinkColumn("Link",display_text="Link")})
        Session['google_results'] = Google_Results
        #-------------------------------------------------------------------------------------------------------------------------------
    
    except Exception as e:
        st.warning(e)
####################################################################################################################################


####################################################################################################################################
# Scrape Google Results
####################################################################################################################################   
if 'google_results' in Session.keys():
    with col4:
        Start_Scapping = st.button('Start Scapping')
    if Start_Scapping:
        #---------------------------------------------------------------------------------------------------------------------------
        # Styling
        #---------------------------------------------------------------------------------------------------------------------------
        st.divider()
        st.subheader('Scrapping Results',divider='rainbow')
        #---------------------------------------------------------------------------------------------------------------------------
        
        #---------------------------------------------------------------------------------------------------------------------------
        # Get Links from Google Results
        #---------------------------------------------------------------------------------------------------------------------------
        Links = Session['google_results']['Link']
        #---------------------------------------------------------------------------------------------------------------------------
        
        #---------------------------------------------------------------------------------------------------------------------------
        # Itterate over each link to scrape website and save in Scrapped_Websites_List
        #---------------------------------------------------------------------------------------------------------------------------
        Scraped_Websites_List = []
        
        for link in stqdm(Links):
            Website_dict = Scrappers.Site_Scrapper(link,included_keywords)
            Scraped_Websites_List.append(Website_dict)
            
        #---------------------------------------------------------------------------------------------------------------------------
        
        #---------------------------------------------------------------------------------------------------------------------------
        # Create a Dataframe for scrapping results and save in Session   
        #---------------------------------------------------------------------------------------------------------------------------
        Scraped_Websites_DF = pd.DataFrame(Scraped_Websites_List)
        Session['Scraped_Websites_DF'] = Scraped_Websites_DF
        #---------------------------------------------------------------------------------------------------------------------------
        
    
        #---------------------------------------------------------------------------------------------------------------------------
        # Merge Scraped Websites with Google Results and Format the findings
        #---------------------------------------------------------------------------------------------------------------------------
        Final_Results = Scrappers.Result_Formating(Session['google_results'],Scraped_Websites_DF,included_keywords)
        #---------------------------------------------------------------------------------------------------------------------------
        
        
        #---------------------------------------------------------------------------------------------------------------------------
        # Print Results
        #---------------------------------------------------------------------------------------------------------------------------
        st.data_editor(Final_Results,column_config={"Link": st.column_config.LinkColumn("Link",display_text="Link")})
        #---------------------------------------------------------------------------------------------------------------------------
        
        
        #---------------------------------------------------------------------------------------------------------------------------
        # Save Data in Session
        #---------------------------------------------------------------------------------------------------------------------------
        Session['Final_Results'] = Final_Results
            
              
####################################################################################################################################
# Download Results and reset search
####################################################################################################################################  
if 'Final_Results' in Session.keys():   
    with col5:
        buffer = io.BytesIO()
        df = Session['Final_Results']
        with pd.ExcelWriter(buffer, engine='xlsxwriter') as writer:
            df.to_excel(writer, sheet_name='Sheet1', index=False)

        Donwload = st.download_button('Download Results',data=buffer,file_name="Results.xlsx",mime='textcsv')
        Session['Download'] = True
 
       
if 'Download' in Session.keys(): 
    with col6:
        Refresh = st.button('Reset Research')
        if Refresh:
            for key in Session.keys():
                del Session[key]